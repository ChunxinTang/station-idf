{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load necessary libraries\n",
    "%matplotlib inline\n",
    "from constructIDF import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read file with precipitation time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook uses as input precipitation time series previously obtained from the National Oceanic and Atmospheric Administration National Centers for Environmental Information accessed [here](https://www.ncei.noaa.gov/data/coop-hourly-precipitation/v2/)). \n",
    "\n",
    "The data from the NCEI needs to be reformatted previously to run this notebook using the\n",
    "`Reformat_NCEI_data` jupyter notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"[*]\"\n",
    "\n",
    "# Specify path to hourly rainfall time series\n",
    "\n",
    "#path = r\"/Users/climate_class/this/is/an/example/path/output_from_Reformat_NCEI_data_notebook.csv\" ###MAC\n",
    "path = r\"\\Users\\climate_class\\this\\is\\an\\example\\path\\output_from_Reformat_NCEI_data_notebook.csv\" ###WINDOWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data into a dataframe and examine.\n",
    "Precipitation time series are organized by date and record, with flags inherited from the NCEI data. For more information on the flag meanings, visit\n",
    "https://www.ncei.noaa.gov/data/coop-hourly-precipitation/v2/doc/readme.csv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = pd.read_csv(path, index_col=0)\n",
    "station_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Construct Annual Maximum Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF curves are constructed based on the Annual Maximum Series (AMS) or Partial Duration Series (PDS). In this tutorial, and given the functions implemented in `constructIDF` we will construct IDF based on AMS. The AMS corresponds to the maximum rainfall value accumulated during a specific duration for each year in record. We want to construct IDF curves for longer durations than one hour (e.g. 2 hours, 6 hours, etc.) and we can do so by aggregating the series above to the corresponding duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify durations for which IDF curves will be created\n",
    "# From one hour, to 3 days.\n",
    "\n",
    "durations = [24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat station data and specify durations to compute AMS\n",
    "ts = AMS(path, durations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the data that will be fed into the AMS methods.\n",
    "ts.reformatted_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AMS for each duration specified above. There are two methods implemented in `constructIDF`: *fixed maxima* and *sliding maxima* algorithms. The algorithms are provided in \n",
    "\n",
    "> Papalexiou, S. M., Dialynas, Y. G., & Grimaldi, S. (2016). Hershfield factor revisited: Correcting annual maximum precipitation. Journal of Hydrology, 542, 884â€“895. https://doi.org/10.1016/j.jhydrol.2016.09.058\n",
    "\n",
    "These two functions extract the annual maximum precipitation (AMS) from a precipitation time series. The two approaches arise from the need to account for the fact that precipitation is systematicall recorded. For example, at a meteorological station, someone will check the tipping bucket pluviographs at some fixed local time each day which is then recorded as a \"daily rainfall time series\" at a particular location. This case results in \"fixed\" records, but \"fixed\" records have been shown inappropriate for estimating rainfall maxima. Because rainfall is a continuous variable, discretizing it can result in biases when estimating extreme rainfall, so it is advised to estimate annual maximum series using the sliding maxima approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts.reformatted_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AMS for each duration specified above.\n",
    "# AMS can be calculated using sliding maxima. (This might take some time.)\n",
    "out = pd.DataFrame(ts.reformatted_frame.groupby(pd.Grouper(key='date', freq='A')).agg(lambda x: x.max()).val)\n",
    "out['year'] = out.index.year\n",
    "out.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#               #\n",
    "#  AMS TABLE    #\n",
    "#               #\n",
    "#################\n",
    "\n",
    "\n",
    "## Take a look at the computed AMS for each duration\n",
    "\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit Generalized Extreme Value and obtain rainfall depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to fit a generalized extreme value distribution to each duration's AMS. Once the parameters (location, scale and shape) are estimated, these are used to retrieve the return levels (in this case, rainfall depth) for different quantiles feed into the inverse of the CDF. Usually, the quantiles are equal to the inverse of the average recurrence interval (ARI) (e.g. 1/2 = 2-year).\n",
    "\n",
    "`constructIDF` has one method that merges all these steps, but we need to specify if we want to construct confidence intervals. The method implemented in `constructIDF` is bootstrapping, so we also need to specify the number of bootsrapped samples. Default value is 1000, and while using a smaller number is not recommended, a larger number of bootstrapped samples increases the time it takes for this cell to run.\n",
    "\n",
    "Other specification is the confidence level, alpha, used to estimate the confidence intervals. Default is 0.9 (90% confidence interval).\n",
    "\n",
    "\n",
    "In this tutorial, we will compute confidence intervals at a 90% confidence level using 50 bootstrapped samples.\n",
    "\n",
    "```python\n",
    "ci = True\n",
    "alpha = 0.9\n",
    "number_bootstrap = 50 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying values\n",
    "ci = True \n",
    "alpha = 0.9\n",
    "number_bootstrap = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"[~~~]\"\n",
    "\n",
    "# Feeding the data and our specifications to the method.\n",
    "\n",
    "data = IDF(out, ci, number_bootstrap, alpha)\n",
    "\n",
    "# Construct IDF from the data we feed above and our specifications.\n",
    "# Some errors will be displayed, no worries. This will take long time because\n",
    "# of the number of bootsrapped samples.\n",
    "# The constructed IDF is by default for the following ARI:\n",
    "# 2-, 5-, 10-, 25-, 50-, 100-, 200-year\n",
    "\n",
    "data.construct_IDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#################\n",
    "#               #\n",
    "#  IDF TABLE    #\n",
    "#               #\n",
    "#################\n",
    "\n",
    "\n",
    "data.idf\n",
    "data.idf_t = data.idf.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generate IDF curves and store IDF values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the next cells to create the IDF curves and plot them.\n",
    "We need to pass the path where the original data was stored, a path where to store\n",
    "the figure and its format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Specify a path where to store figure, and the figure format\n",
    "\n",
    "Specify the path and format in the cell below between quotation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"[*]\"\n",
    "\n",
    "figure_path = \"\"\n",
    "figure_name = \"fut_station_idf_plot\"\n",
    "figure_format = \"jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Store values from IDF table\n",
    "\n",
    "The code below saves the IDF table (`data.idf`) to a csv file in the specified path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"[*]\"\n",
    "#data.idf.to_csv(r\"/Users/climate_class/Example/Path/hist_station_idf.csv\") ###MAC\n",
    "data.idf.to_csv( r\"\\Users\\climate_class\\Example\\Path\\hist_station_idf.csv\") ###WINDOWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = data.idf_t.drop([x for x in data.idf_t.columns if 'L' in x], axis=1)\n",
    "median = median.drop([x for x in median.columns if 'U' in x], axis=1)\n",
    "data.idf.columns = ['24H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_level = data.idf_t.drop([x for x in data.idf_t.columns if 'U' not in x], axis=1)\n",
    "lower_level = data.idf_t.drop([x for x in data.idf_t.columns if 'L' not in x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Plot the IDF Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line generates a plot and saves in your folder specified above\n",
    "rcParams['xtick.labelsize'] = 14\n",
    "rcParams['ytick.labelsize'] = 14\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(10, 7))\n",
    "median.transpose().plot(ax=axs, label='median', c='k')\n",
    "plt.fill_between(np.arange(0, 7), lower_level.transpose().val.values,\n",
    "                         upper_level.transpose().val.values, alpha=0.3)\n",
    "plt.xticks(np.arange(0, 7), ('2', '5', '10',\n",
    "                                         '25', '50', '100', '200'))\n",
    "plt.legend(['Median', '95% CI'], fontsize = 18, loc=2)\n",
    "plt.xlabel('Return Period (years)', fontsize=18)\n",
    "plt.ylabel('Precipitation Depth (in)', fontsize=18)\n",
    "plt.title('Pittsburgh Historical 24-hour DDF curve', fontsize=22)\n",
    "\n",
    "\n",
    "#if figure_path[-1] == \"/\": ###MAC\n",
    "if figure_path[-1] == r\"\\\\\": ###WINDOWS\n",
    "    path_save_fig ='{}{}.{}'.format(figure_path, figure_name, figure_format)\n",
    "else:\n",
    "    #path_save_fig ='{}/{}.{}'.format(figure_path, figure_name, figure_format) ###MAC\n",
    "    path_save_fig ='{}\\{}.{}'.format(figure_path, figure_name, figure_format) ###WINDOWS\n",
    "\n",
    "#print(path_save_fig) #I added this line\n",
    "plt.savefig(path_save_fig, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
